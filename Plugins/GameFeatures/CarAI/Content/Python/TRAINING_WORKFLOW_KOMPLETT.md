# Kompletter Training-Workflow - Alles was du wissen musst

## ğŸ“‹ Ãœbersicht: Wie funktioniert das Training?

Das Training lÃ¤uft in **zwei Phasen**, die du abwechselnd wiederholst:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: Daten sammeln (in Unreal)                          â”‚
â”‚  â†’ Agents fahren mit aktuellem Modell                       â”‚
â”‚  â†’ Observations/Actions/Rewards werden gesammelt            â”‚
â”‚  â†’ Rollouts werden als JSON exportiert                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: Modell trainieren (in Python)                      â”‚
â”‚  â†’ PyTorch trainiert das Modell mit gesammelten Daten       â”‚
â”‚  â†’ Mehrere Epochen werden gespeichert (epoch_1 bis epoch_10)â”‚
â”‚  â†’ Bestes Modell wird als JSON exportiert                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: Modell zurÃ¼ckladen (in Unreal)                     â”‚
â”‚  â†’ Trainiertes Modell wird in Unreal geladen                â”‚
â”‚  â†’ Agents fahren jetzt mit besserem Modell                  â”‚
â”‚  â†’ Bessere Daten werden gesammelt â†’ zurÃ¼ck zu Phase 1       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Wichtig:** Dies ist ein **iterativer Prozess**. Du wiederholst diese Phasen, bis die Agents gut fahren!

---

## ğŸš€ Phase 1: Daten in Unreal sammeln

### Vorbereitung

1. **Widget Ã¶ffnen:**
   - Content Browser â†’ `EUW_RacingTraining` â†’ Rechtsklick â†’ `Run Editor Utility Widget`

2. **Konfiguration prÃ¼fen:**
   - `bExportOnly = true` (WICHTIG fÃ¼r Datensammlung!)
   - `Agent Pawn Class`: Dein Car Pawn
   - `Num Agents`: 4-10 (mehr = schneller, aber mehr Performance nÃ¶tig)

3. **RacingAgentComponent Parameter prÃ¼fen** (siehe unten)

### Workflow

1. **Play drÃ¼cken** (wÃ¤hrend Widget offen ist)
2. **Spawn Agents** (falls noch nicht gespawnt)
3. **Initialize Training** (wÃ¤hrend Play)
4. **Start Training** (wÃ¤hrend Play)

### Was passiert?

- Agents fahren mit **aktuellem Modell** (beim ersten Mal: zufÃ¤llig initialisiert)
- Nach jedem **Rollout** (2048 Steps) werden Daten gesammelt
- Daten werden in `Saved/Training/Exports/rollout_*.json` gespeichert
- **Kein PPO-Training** findet statt (nur Datensammlung!)

### Wann aufhÃ¶ren?

**Empfehlung:**
- **Beim ersten Training:** 10-20 Rollouts sammeln
- **Bei spÃ¤teren Iterationen:** 5-10 Rollouts reichen (das Modell ist bereits besser)

**Stoppe das Training**, wenn:
- Du genug Rollouts gesammelt hast
- Die Agents zeigen erste Fortschritte (optional, aber ermutigend)

### âš ï¸ Wichtige Frage: Alte Exports lÃ¶schen?

**Kurze Antwort:** Meistens **NEIN** - behalte alte Exports!

**Das Training-Script verwendet ALLE Rollout-Dateien** im Export-Verzeichnis. Du hast zwei Optionen:

#### Option 1: Alle Exports behalten (Empfohlen fÃ¼r kontinuierliches Training)

**Vorteile:**
- âœ… Mehr Daten = stabileres Training
- âœ… Kontinuierliches Lernen aus allen bisherigen Daten
- âœ… Bessere Generalisierung

**Nachteile:**
- âŒ Alte Daten kÃ¶nnen veraltet sein (wenn sich Reward-Shaping geÃ¤ndert hat)
- âŒ Training wird langsamer (mehr Daten)

**Wann verwenden:**
- âœ… Beim ersten Training (alle Daten behalten)
- âœ… Bei spÃ¤teren Iterationen (alte + neue Daten)
- âœ… Wenn sich nichts Grundlegendes geÃ¤ndert hat

#### Option 2: Alte Exports lÃ¶schen (FÃ¼r "frischen Start")

**Vorteile:**
- âœ… Nur aktuelle Daten (passen zum aktuellen Modell)
- âœ… Schnelleres Training
- âœ… Klarerer Start nach grÃ¶ÃŸeren Ã„nderungen

**Nachteile:**
- âŒ Weniger Daten â†’ mÃ¶glicherweise instabileres Training
- âŒ Verliert bisheriges Wissen

**Wann verwenden:**
- âœ… Wenn du Reward-Shaping **stark** geÃ¤ndert hast
- âœ… Wenn sich die Strecke **stark** geÃ¤ndert hat
- âœ… Wenn Training zu langsam wird (zu viele Dateien)
- âœ… Wenn du einen "frischen Start" testen willst

**Wie alte Exports lÃ¶schen:**
```powershell
# Alle Rollout-Dateien lÃ¶schen (VORSICHT!)
Remove-Item "C:\Users\YvesT\Documents\Unreal Projects\StuntCarRacer\Saved\Training\Exports\rollout_*.json"

# Oder nur alte Dateien (z.B. vor einem bestimmten Datum)
Get-ChildItem "C:\Users\YvesT\Documents\Unreal Projects\StuntCarRacer\Saved\Training\Exports\rollout_*.json" | 
    Where-Object { $_.LastWriteTime -lt (Get-Date).AddDays(-7) } | 
    Remove-Item
```

**Empfehlung:**
- **Beim ersten Training:** Alle Exports behalten
- **Bei spÃ¤teren Iterationen:** Alte Exports behalten (es sei denn, du hast etwas Grundlegendes geÃ¤ndert)
- **Bei Problemen:** Teste einen "frischen Start" (alte Exports lÃ¶schen)

---

## ğŸ Phase 2: Python-Training

### Vorbereitung

1. **PrÃ¼fe, dass Rollouts existieren:**
   ```
   Saved/Training/Exports/rollout_*.json
   ```

2. **Python-Script starten:**
   ```powershell
   cd "C:\Users\YvesT\Documents\Unreal Projects\StuntCarRacer\Plugins\GameFeatures\CarAI\Content\Python"
   python train_pytorch.py
   ```

### Was passiert?

- Script liest alle `rollout_*.json` Dateien
- Trainiert ein PPO-Modell Ã¼ber **10 Epochen** (standardmÃ¤ÃŸig)
- Speichert nach jeder Epoche: `model_epoch_1.pt`, `model_epoch_2.pt`, ..., `model_epoch_10.pt`
- Dateien werden in `Saved/Training/Models/` gespeichert

### Welche Epoche soll ich wÃ¤hlen?

**Empfehlung fÃ¼r AnfÃ¤nger:**
- Nimm **immer die letzte Epoche** (`model_epoch_10.pt`)
- Warum? Meistens ist das die am besten trainierte Version

**FÃ¼r Fortgeschrittene:**
- Schau dir die **Loss-Werte** im Training an:
  - **Value Loss** sollte sinken (z.B. von 1.5 auf 0.5)
  - **Policy Loss** sollte stabil bleiben oder leicht sinken
- Falls Loss-Werte spÃ¤ter wieder steigen â†’ **Overfitting**! Nimm eine frÃ¼here Epoche (z.B. `epoch_7`)
- Falls Loss-Werte kontinuierlich sinken â†’ Nimm die letzte Epoche (`epoch_10`)

**Wann verschiedene Epochen testen?**
- Wenn das Modell nach dem Laden **schlechter** wird â†’ Teste frÃ¼here Epochen
- Wenn das Training **instabil** war â†’ Teste mittlere Epochen (z.B. `epoch_5`)

---

## ğŸ“¥ Phase 3: Modell zurÃ¼ck in Unreal laden

### Schritt 1: Modell zu JSON exportieren

```powershell
cd "C:\Users\YvesT\Documents\Unreal Projects\StuntCarRacer\Plugins\GameFeatures\CarAI\Content\Python"
python export_model_for_unreal.py "C:\Users\YvesT\Documents\Unreal Projects\StuntCarRacer\Saved\Training\Models\model_epoch_10.pt" "C:\Users\YvesT\Documents\Unreal Projects\StuntCarRacer\Saved\Training\Models\model_epoch_10.json"
```

### Schritt 2: In Unreal laden

1. **Widget Ã¶ffnen** (falls nicht offen)
2. **Checkpoint Filename:** `model_epoch_10.json`
3. **Load Checkpoint** klicken
4. Du solltest sehen: "Checkpoint loaded: ..."

### Was passiert?

- Das trainierte Modell wird in die `PolicyNetwork` geladen
- Agents verwenden jetzt dieses Modell fÃ¼r ihre Actions
- Beim nÃ¤chsten Training werden **bessere Daten** gesammelt

---

## ğŸ”„ Der komplette Zyklus (Beispiel)

### Iteration 1: Erstes Training

1. **Daten sammeln:** 20 Rollouts (Agents fahren zufÃ¤llig/schlecht)
2. **Python-Training:** `train_pytorch.py` â†’ `model_epoch_10.pt`
3. **Modell laden:** `model_epoch_10.json` in Unreal
4. **Ergebnis:** Agents fahren etwas besser, aber noch nicht gut

### Iteration 2: Zweites Training

1. **Daten sammeln:** 10 Rollouts (Agents fahren mit Iteration-1-Modell)
2. **Python-Training:** Neue Daten + alte Daten â†’ `model_epoch_10.pt` (v2)
3. **Modell laden:** Neues Modell in Unreal
4. **Ergebnis:** Agents fahren deutlich besser

### Iteration 3, 4, 5...: Weiter verbessern

- Wiederhole den Zyklus
- Mit jedem Zyklus werden die Agents besser
- Irgendwann: Agents kÃ¶nnen die ganze Strecke fahren!

---

## âš™ï¸ RacingAgentComponent: Welche Parameter muss ich setzen?

### âŒ NICHT dynamisch (musst du manuell setzen):

Diese Werte sind **statisch** und mÃ¼ssen im Blueprint oder Component gesetzt werden:

#### **Racing|Wiring:**
- **Track Provider:** Muss auf `TrackFrameProviderComponent` zeigen
- **Curriculum Asset:** Optional, fÃ¼r Curriculum Learning

#### **Racing|Obs (Observation-Normalisierung):**
- **Track Half Width Cm:** `500.0` (Standard) - Breite der Strecke
- **Heading Norm Rad:** `0.8` (Standard) - Normalisierung fÃ¼r Heading
- **Speed Norm Cm Per Sec:** `4500.0` (Standard) - Normalisierung fÃ¼r Geschwindigkeit
- **Ang Vel Norm Deg Per Sec:** `220.0` (Standard) - Normalisierung fÃ¼r Winkelgeschwindigkeit
- **Curvature Norm Inv Cm:** `0.0025` (Standard) - Normalisierung fÃ¼r Kurvenradius
- **Lookahead Offsets Cm:** `[200, 600, 1200, 2000]` (Standard) - Lookahead-Punkte

**Wann Ã¤ndern?**
- Nur wenn deine Strecke **sehr anders** ist (z.B. viel schmaler/breiter)
- Standard-Werte funktionieren fÃ¼r die meisten FÃ¤lle

#### **Racing|Reward (Reward-Shaping):**
- **RewardCfg:** Komplette Reward-Konfiguration (siehe unten)

**Wann Ã¤ndern?**
- Wenn Agents **zu aggressiv** fahren â†’ ErhÃ¶he `W_Lateral` (negativer = mehr Bestrafung fÃ¼r seitliches Abweichen)
- Wenn Agents **zu langsam** fahren â†’ ErhÃ¶he `W_Speed`
- Wenn Agents **vom Track fliegen** â†’ ErhÃ¶he `W_Lateral` und `W_Heading`

#### **Racing|Reset (Auto-Reset):**
- **Stuck Min Progress Cm Per Sec:** `120.0` (Standard) - Wenn Agent langsamer â†’ Reset
- **Stuck Time Seconds:** `1.25` (Standard) - Wie lange "stuck" sein, bevor Reset
- **Landing Grace Seconds:** `0.6` (Standard) - Grace-Period nach Landung
- **Teleport Grace Seconds:** `0.6` (Standard) - Grace-Period nach Teleport

**Wann Ã¤ndern?**
- Wenn Agents zu frÃ¼h resettet werden â†’ ErhÃ¶he `Stuck Time Seconds`
- Wenn Agents zu lange "stuck" bleiben â†’ Verringere `Stuck Time Seconds`

### âœ… Dynamisch (werden automatisch angepasst):

Diese Werte werden **vom Training Manager** gesetzt:

- **Actions** (Steer, Throttle, Brake) - Kommen vom Neural Network
- **Observations** - Werden automatisch aus Car-State berechnet
- **Rewards** - Werden automatisch basierend auf `RewardCfg` berechnet

**Du musst diese NICHT setzen!**

---

## ğŸ¯ Reward-Shaping: Die wichtigsten Parameter

Die `RewardCfg` im `RacingAgentComponent` bestimmt, **was** die Agents lernen sollen:

### âš™ï¸ Wie funktionieren die Werte?

**Wichtig zu verstehen:**
- `W_Lateral` und `W_Heading` sind **negativ** (Bestrafung)
- Die tatsÃ¤chliche Bestrafung wird **quadriert** berechnet: `W_Lateral * (LateralNorm * LateralNorm)`
- Das bedeutet: **Doppelte Abweichung = 4x Bestrafung** (exponentiell!)
- Daher reichen schon moderate Werte (-0.25 bis -0.4) fÃ¼r die meisten FÃ¤lle

**Beispiel:**
- `W_Lateral = -0.25` mit `LateralNorm = 0.5` â†’ Bestrafung = -0.25 * 0.25 = **-0.0625**
- `W_Lateral = -0.25` mit `LateralNorm = 1.0` â†’ Bestrafung = -0.25 * 1.0 = **-0.25** (4x stÃ¤rker!)

### Standard-Werte (empfohlen fÃ¼r den Start):

```cpp
// Terminal Penalties (wenn Episode endet)
TerminalPenalty_OffTrack = -2.0      // Strafe fÃ¼r "von Strecke fliegen"
TerminalPenalty_WrongWay = -2.0      // Strafe fÃ¼r "falsche Richtung"
TerminalPenalty_Stuck = -2.0         // Strafe fÃ¼r "stecken bleiben"

// Reward Weights (kontinuierliche Belohnungen)
W_Progress = 1.0                     // Belohnung fÃ¼r Fortschritt (HOCH!)
W_Speed = 0.2                        // Belohnung fÃ¼r Geschwindigkeit
W_Lateral = -0.15                    // Bestrafung fÃ¼r seitliches Abweichen
W_Heading = -0.15                    // Bestrafung fÃ¼r falsche Ausrichtung
W_AngVel = -0.05                     // Bestrafung fÃ¼r schnelle Rotation
W_ActionSmooth = -0.02               // Belohnung fÃ¼r sanfte Actions
W_Airborne = -0.05                   // Bestrafung fÃ¼r "in der Luft"
```

### Wenn Agents von der Strecke fliegen:

**Problem:** Agents verlassen die Strecke zu oft

**Wichtig:** Die Werte werden **quadriert** verwendet (`LateralNorm * LateralNorm`), daher steigt die Bestrafung exponentiell mit der Abweichung.

**Schrittweise Anpassung (empfohlen):**

#### Schritt 1: Leichtes Problem (Agents weichen gelegentlich ab)
```cpp
W_Lateral = -0.25                    // Standard: -0.15 â†’ erhÃ¶he auf -0.25
W_Heading = -0.25                    // Standard: -0.15 â†’ erhÃ¶he auf -0.25
TerminalPenalty_OffTrack = -3.0      // Standard: -2.0 â†’ erhÃ¶he auf -3.0
```

#### Schritt 2: Mittleres Problem (Agents fliegen regelmÃ¤ÃŸig ab)
```cpp
W_Lateral = -0.4                     // ErhÃ¶he auf -0.4
W_Heading = -0.4                     // ErhÃ¶he auf -0.4
TerminalPenalty_OffTrack = -5.0     // ErhÃ¶he auf -5.0
```

#### Schritt 3: Starkes Problem (Agents fliegen sehr oft ab)
```cpp
W_Lateral = -0.6                     // ErhÃ¶he auf -0.6
W_Heading = -0.6                     // ErhÃ¶he auf -0.6
TerminalPenalty_OffTrack = -8.0     // ErhÃ¶he auf -8.0
```

#### Schritt 4: Extremes Problem (Agents schaffen keine Kurve)
```cpp
W_Lateral = -0.8                     // ErhÃ¶he auf -0.8 (Maximum empfohlen)
W_Heading = -0.8                     // ErhÃ¶he auf -0.8 (Maximum empfohlen)
TerminalPenalty_OffTrack = -10.0    // ErhÃ¶he auf -10.0
```

**âš ï¸ Wichtig:**
- **Nicht zu hoch setzen!** Werte Ã¼ber -1.0 kÃ¶nnen das Training instabil machen
- **Schrittweise erhÃ¶hen:** Starte mit Schritt 1, teste, dann weiter
- **Balance beachten:** Wenn `W_Lateral`/`W_Heading` zu hoch sind, werden Agents zu vorsichtig und fahren zu langsam

### Wenn Agents zu langsam fahren:

**Problem:** Agents fahren zu vorsichtig

**LÃ¶sung:**
```cpp
W_Speed = 0.4                        // ErhÃ¶he (mehr Belohnung fÃ¼r Geschwindigkeit)
W_Progress = 1.5                     // ErhÃ¶he (mehr Belohnung fÃ¼r Fortschritt)
```

### Wenn Agents zu aggressiv fahren:

**Problem:** Agents crashen zu oft

**LÃ¶sung:**
```cpp
W_ActionSmooth = -0.05               // ErhÃ¶he (negativer = mehr Bestrafung fÃ¼r abrupte Actions)
W_AngVel = -0.1                      // ErhÃ¶he (negativer = mehr Bestrafung fÃ¼r schnelle Rotation)
```

---

## ğŸš— Warum schaffen die Autos keine Kurven?

### MÃ¶gliche Ursachen:

1. **Zu wenig Training:**
   - LÃ¶sung: Mehr Iterationen durchfÃ¼hren (3-5 Zyklen)
   - Mehr Rollouts sammeln (20+ beim ersten Mal)

2. **Reward-Shaping nicht optimal:**
   - LÃ¶sung: Passe `W_Lateral` und `W_Heading` an (siehe oben)

3. **Observation-Normalisierung falsch:**
   - LÃ¶sung: PrÃ¼fe `LookaheadOffsetsCm` - sollten 4 Werte sein: `[200, 600, 1200, 2000]`

4. **Modell noch nicht gut genug:**
   - LÃ¶sung: Trainiere lÃ¤nger (mehr Epochen in Python)
   - Sammle mehr Daten

5. **Action Noise zu hoch:**
   - LÃ¶sung: PrÃ¼fe `ActionNoise` im Training Manager (sollte mit der Zeit sinken)

### Debugging-Tipps:

1. **Aktiviere Debug-Visualisierung:**
   - `RacingAgentComponent` â†’ `bDrawObservationDebug = true`
   - Zeigt Lookahead-Punkte und Observations

2. **PrÃ¼fe Rewards:**
   - `RacingAgentComponent` â†’ `bDrawRewardHUD = true`
   - Zeigt aktuelle Rewards wÃ¤hrend Fahren

3. **PrÃ¼fe Console-Logs:**
   - Suche nach "Reward", "OffTrack", "Stuck"
   - Zeigt, warum Episoden enden

---

## ğŸ“Š Wann ist das Training gut genug?

### Gute Zeichen:

- âœ… Agents fahren **lÃ¤nger** auf der Strecke
- âœ… **Reward MA100** steigt kontinuierlich
- âœ… **Progress MA100** steigt (Agents kommen weiter)
- âœ… **Weniger "OffTrack" Terminierungen**
- âœ… Agents kÃ¶nnen **einfache Kurven** fahren

### Schlechte Zeichen:

- âŒ Reward MA100 **stagniert** oder sinkt
- âŒ Agents werden **nicht besser** nach mehreren Iterationen
- âŒ **Value Loss** steigt im Python-Training (Overfitting!)

### Wann stoppen?

- Wenn Agents die **ganze Strecke** schaffen
- Wenn Agents **konsistent gut** fahren (Ã¼ber mehrere Iterationen)
- Wenn du mit dem Ergebnis **zufrieden** bist

---

## ğŸ”§ HÃ¤ufige Probleme und LÃ¶sungen

### Problem: "Agents fahren gar nicht"

**Ursachen:**
- `bEnableDriving = false` im RacingAgentComponent
- Policy Network nicht initialisiert
- Actions werden nicht angewendet

**LÃ¶sung:**
- PrÃ¼fe `bEnableDriving = true`
- PrÃ¼fe, ob Training initialisiert wurde
- PrÃ¼fe Console fÃ¼r Fehler

### Problem: "Agents fahren nur geradeaus"

**Ursachen:**
- Action Noise zu niedrig
- Modell noch nicht trainiert (erste Iteration)
- Observations nicht korrekt

**LÃ¶sung:**
- PrÃ¼fe Action Noise (sollte beim ersten Training hoch sein: 0.5-1.0)
- Warte auf mehr Training-Iterationen
- Aktiviere `bDrawObservationDebug` und prÃ¼fe Observations

### Problem: "Training ist zu langsam"

**Ursachen:**
- Zu viele Agents
- Rollout Steps zu hoch
- DeltaTime zu niedrig

**LÃ¶sung:**
- Reduziere Anzahl Agents (4-6 statt 10)
- Reduziere Rollout Steps (1024 statt 2048)
- ErhÃ¶he Tick-Rate (falls mÃ¶glich)

---

## ğŸ“ Checkliste fÃ¼r jeden Trainings-Zyklus

### Vor dem Training:

- [ ] Widget geÃ¶ffnet und konfiguriert
- [ ] `bExportOnly = true` (fÃ¼r Datensammlung)
- [ ] RacingAgentComponent Parameter geprÃ¼ft
- [ ] Track Provider gesetzt
- [ ] Reward-Konfiguration angepasst (falls nÃ¶tig)

### WÃ¤hrend des Trainings:

- [ ] Play gestartet
- [ ] Agents gespawnt
- [ ] Training initialisiert
- [ ] Training gestartet
- [ ] Rollouts werden gesammelt (Logs prÃ¼fen)

### Nach dem Training:

- [ ] Training gestoppt
- [ ] Rollout-Dateien vorhanden (`Saved/Training/Exports/`)
- [ ] Python-Training gestartet
- [ ] Modell exportiert (`export_model_for_unreal.py`)
- [ ] Modell in Unreal geladen
- [ ] Ergebnis getestet (Play â†’ beobachten)

---

## ğŸ“ Zusammenfassung: Die wichtigsten Punkte

1. **Training ist iterativ:** Unreal sammelt Daten â†’ Python trainiert â†’ Modell zurÃ¼ck â†’ besser Daten sammeln â†’ ...

2. **Erste Iteration ist am wichtigsten:** Beim ersten Mal fahren Agents zufÃ¤llig - sammle viele Rollouts (20+)

3. **Nimm die letzte Epoche:** `model_epoch_10.pt` ist meistens das beste Modell

4. **RacingAgentComponent Parameter:**
   - **Reward-Konfiguration** ist wichtig (passe `W_Lateral`, `W_Heading`, `W_Speed` an)
   - **Observation-Normalisierung** meist Standard (nur bei sehr unterschiedlichen Strecken Ã¤ndern)
   - **Reset-Parameter** kÃ¶nnen angepasst werden (falls Agents zu frÃ¼h/spÃ¤t resettet werden)

5. **Reward-Shaping ist wichtig:**
   - Wenn Agents von Strecke fliegen â†’ ErhÃ¶he `W_Lateral` und `W_Heading` (negativer)
   - Wenn Agents zu langsam â†’ ErhÃ¶he `W_Speed` und `W_Progress`

6. **Geduld:** RL-Training braucht Zeit - mehrere Iterationen sind normal!

---

Viel Erfolg beim Training! ğŸš—ğŸ¤–
